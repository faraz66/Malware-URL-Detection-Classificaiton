# Malware URL Detection and Classification (Severity-based)

This project focuses on detecting and classifying URLs using machine learning models, with a special emphasis on classifying malware into severity levels. By leveraging key lexical features extracted from URLs, this project categorizes them into benign, malware, phishing, and more. The dataset used is from Phishtank, a well-known source for phishing URLs.

# Table of Contents
    1.Introduction
    2.Dataset
    3.Features
    4.Machine Learning Models
    5.Results
    6.Running the Project
    7.Installation
    8.Contributing
    9.License

# Introduction

The detection of malicious URLs is crucial in preventing phishing attacks, malware downloads, and unauthorized website modifications. This project uses machine learning to classify URLs based on their threat level, including categorizing malware into severity levels 

    Low
    
    Medium
    
    High 

The primary objective is to enhance cybersecurity by enabling real-time URL classification.

# Dataset

The dataset used for this project is sourced from Phishtank, which provides verified phishing URLs. The dataset is enriched with additional lexical and host-based features, allowing us to classify URLs into categories such as:

Benign: Safe URLs.

Phishing: URLs attempting to steal sensitive information.

Malware: Further classified based on severity levels (low, medium, high).

# Features
We extract several key features from the URLs to help classify them, including:

List of Features with Descriptions

        1. phish_id: Unique identifier for each phishing URL in the dataset.
         
        2. url: The full URL that is being analyzed for phishing or malicious behavior.
        
        3. verified: A boolean value indicating whether the URL has been verified as malicious or safe.
        
        4. online: A boolean flag representing whether the URL is currently active or online.
        
        5. target: The organization or entity that is the primary target of the phishing URL.
        
        6. subdomain: The subdomain extracted from the URL, providing additional details about the URL structure.
        
        7. tld: Top-Level Domain (TLD) of the URL, e.g., .com, .org.
        
        8. sld: Second-Level Domain (SLD), which identifies the domain name, such as example in example.com.
        
        9. has_subdomain: A boolean value indicating if the URL contains a subdomain.
        
        10. domain_length: The total number of characters in the domain name.
        
        11. numeric_percentage: The percentage of numeric characters present in the URL.
        
        12. char_distribution: The distribution of characters (alphanumeric, special, etc.) within the URL.
        
        13. entropy: A measure of the randomness of characters in the URL, often used to detect obfuscated or suspicious URLs.
        
        14. url_length: The total number of characters in the entire URL.
        
        15. subdomain_count: The number of subdomains in the URL, which can indicate complexity or attempts to obfuscate.
        
        16. has_suspicious_keywords: A flag indicating the presence of suspicious keywords like "login", "secure", "account", "bank", etc., in the URL.
        
        17. special_char_count: The total count of special characters like @, %, &, = that appear in the URL.
        
        18. Registrar: The domain registrar responsible for registering the domain.
        
        19. Registrant Name: The name of the individual or organization who registered the domain.
        
        20. Emails: The email addresses associated with the domain registration.
        
        21. Domain Age (years): The age of the domain in years, which is calculated based on the WHOIS data.
        
        22. Organization: The organization that registered the domain.
        
        23. State: The state where the domain is registered.
        
        24. Country: The country where the domain is registered.

        25. Name Server Count: The number of name servers associated with the domain.

        26. longest_word: The longest token or word within the URL, potentially indicating complex or obfuscated URLs.
        
        27. typosquatting: A score or flag indicating if the URL is involved in typosquatting, which involves registering domains that are close in spelling to legitimate sites.
        
        28. obfuscation_score: A score measuring the likelihood that the URL is obfuscated to disguise malicious intent.
        
        29. z_score_outlier: The Z-score indicating how much the URL deviates from the statistical mean, used to detect outliers.
        
        30. is_outlier: A boolean indicating whether the URL is classified as an outlier using the Z-score method.
        
        31. outlier_isolation_forest: A flag indicating whether the URL is classified as an outlier using the Isolation Forest algorithm.
        
        32. is_common_outlier: A combined indicator of outliers using both Z-score and Isolation Forest methods.
        
        33. is_combined_outlier: A boolean indicating whether the URL is classified as an outlier using a combined approach.
        
        34. pred_severity_score: The predicted severity score of the URL based on the model, with higher values indicating more dangerous URLs.
        
        35. pred_severity_level: The predicted severity level of the URL, which classifies it into severity categories (e.g., low, medium, high).
        
        36. severity_score: The actual severity score based on external or historical data.
        
        37. severity: The actual severity level, categorized into severity buckets like low, medium, or high.
        
        38. google_safe_browsing_score: A score from Google Safe Browsing that indicates the threat level of the URL.
        
        39. google_safe_browsing_threat: The type of threat detected by Google Safe Browsing, such as phishing or malware.

# Machine Learning Models
 We explored various machine learning models, such as:
 
  1. Random Forest
  
  2. Support Vector Machines
  
  3. Decision Trees
  
Note : Random Forest was found to be the most effective, especially in classifying malware into different severity levels. We applied cross-validation and grid search to fine-tune the model.

# Results

The classification achieved impressive performance:

Accuracy: 96%

Precision: 98% for malware classification

Recall: 95% for severity levels


##  Running the Project

### ðŸ”§ Steps

1. **Clone the Repository**:

```bash
   git clone https://github.com/your_username/malware_url_detection.git
   ```
2. **Navigate to the Project Directory**:

 ```bash
   cd malware_url_detection
   ```

3. **Install Dependencies**:

```bash

pip install -r requirements.txt
```

4. Run the Notebook/Application:

```bash
python app.py
```

Access the web application at http://127.0.0.1:5000/ for real-time URL analysis and severity classification.

# Installation in Python 3.6+
     pandas
     numpy 
     urlparse
     collections
     math
     itertools
     python-whois
     requests
     tldextract
     logging
     python-datetime 
     dateutil
     concurrent.futures
     os
     json
     tldextract
     tempfile
     time
     seaborn
     matplotlib.pyplot 
     google.colab 
We welcome contributions! Please follow the typical fork and pull-request workflow to contribute new features or improvements.

# License
This project is licensed under the MIT License.
